{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "collapsed": true,
    "id": "2VD0STvcPVou"
   },
   "outputs": [],
   "source": [
    "!pip install torchaudio\n",
    "!pip install librosa\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ce1V538SPiq8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import seaborn as sns\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tbqb5IUt3vfa"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-x_mHZeLFcCT"
   },
   "outputs": [],
   "source": [
    "\n",
    "librispeech_dataset = torchaudio.datasets.LIBRISPEECH(\".\", url=\"train-clean-100\", download=True)\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def play_audio(waveform, sample_rate):\n",
    "    display(Audio(waveform.numpy(), rate=sample_rate))\n",
    "sample_waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = librispeech_dataset[0]\n",
    "print(f\"Sample Rate: {sample_rate}\")\n",
    "print(f\"Speaker ID: {speaker_id}, Chapter ID: {chapter_id}, Utterance ID: {utterance_id}\")\n",
    "print(f\"Transcript: {utterance}\")\n",
    "print(f\"Waveform shape: {sample_waveform.shape}\")\n",
    "play_audio(sample_waveform, sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FQW8XJUrmGUR"
   },
   "outputs": [],
   "source": [
    "mfcc_transform = transforms.MFCC(sample_rate=16000, n_mfcc=13)\n",
    "for i in range(10):\n",
    "    sample_waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = librispeech_dataset[i]\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(sample_waveform.numpy(), sr=sample_rate)\n",
    "    plt.title(f'Waveform of Sample {i+1} - Speaker: {speaker_id}')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    mfcc_features = mfcc_transform(sample_waveform)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    librosa.display.specshow(mfcc_features[0].numpy(), sr=sample_rate, x_axis='time')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'MFCCs of Sample {i+1}')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('MFCC Coefficients')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NqttTZ9Dz-fe"
   },
   "outputs": [],
   "source": [
    "def augment_waveform(waveform, sample_rate):\n",
    "    waveform = transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    noise = torch.randn_like(waveform) * 0.005\n",
    "    waveform = waveform + noise\n",
    "    waveform = transforms.Vol(gain=0.5)(waveform)\n",
    "    return waveform\n",
    "\n",
    "def get_mfcc(waveform, sample_rate, max_length=500):\n",
    "    waveform = transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    mfcc_transform = transforms.MFCC(\n",
    "        sample_rate=16000,\n",
    "        n_mfcc=13,\n",
    "        melkwargs={\"n_fft\": 512, \"n_mels\": 80, \"hop_length\": 160}\n",
    "    )\n",
    "    mfcc = mfcc_transform(waveform)\n",
    "    if mfcc.size(-1) > max_length:\n",
    "        mfcc = mfcc[:, :, :max_length]\n",
    "    else:\n",
    "        mfcc = F.pad(mfcc, (0, max_length - mfcc.size(-1)))\n",
    "    mfcc_mean = mfcc.mean(dim=-1, keepdim=True)\n",
    "    mfcc_std = mfcc.std(dim=-1, keepdim=True)\n",
    "    mfcc_normalized = (mfcc - mfcc_mean) / (mfcc_std + 1e-6)\n",
    "    mfcc_normalized = mfcc_normalized.unsqueeze(0)\n",
    "    return mfcc_normalized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xxyKE7wE2idg"
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 16\n",
    "subset_size = 5000\n",
    "indices = random.sample(range(len(librispeech_dataset)), subset_size)\n",
    "train_size = int(0.8 * subset_size)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_dataset = Subset(librispeech_dataset, train_indices)\n",
    "val_dataset = Subset(librispeech_dataset, val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "etjqmo3hDKOo"
   },
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, augment=False):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.speaker_indices = self._build_speaker_indices()\n",
    "        self.triplets = self._generate_triplets()\n",
    "\n",
    "    def _build_speaker_indices(self):\n",
    "        speaker_indices = {}\n",
    "        for i, (_, _, _, speaker_id, _, _) in enumerate(self.dataset):\n",
    "            if speaker_id not in speaker_indices:\n",
    "                speaker_indices[speaker_id] = []\n",
    "            speaker_indices[speaker_id].append(i)\n",
    "        return speaker_indices\n",
    "\n",
    "    def _generate_triplets(self):\n",
    "        triplets = []\n",
    "        speaker_ids = list(self.speaker_indices.keys())\n",
    "\n",
    "        for speaker_id in speaker_ids:\n",
    "            indices = self.speaker_indices[speaker_id]\n",
    "            other_speaker_ids = [id for id in speaker_ids if id != speaker_id]\n",
    "\n",
    "            for anchor_idx in indices:\n",
    "                if len(indices) < 2:\n",
    "                    continue\n",
    "                positive_idx = random.choice([idx for idx in indices if idx != anchor_idx])\n",
    "                negative_speaker_id = random.choice(other_speaker_ids)\n",
    "                negative_idx = random.choice(self.speaker_indices[negative_speaker_id])\n",
    "\n",
    "                triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "        random.shuffle(triplets)\n",
    "        return triplets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_idx, positive_idx, negative_idx = self.triplets[idx]\n",
    "\n",
    "        anchor_waveform, sample_rate, _, _, _, _ = self.dataset[anchor_idx]\n",
    "        positive_waveform, _, _, _, _, _ = self.dataset[positive_idx]\n",
    "        negative_waveform, _, _, _, _, _ = self.dataset[negative_idx]\n",
    "\n",
    "        if self.augment:\n",
    "            anchor_waveform = augment_waveform(anchor_waveform, sample_rate)\n",
    "            positive_waveform = augment_waveform(positive_waveform, sample_rate)\n",
    "            negative_waveform = augment_waveform(negative_waveform, sample_rate)\n",
    "\n",
    "        anchor = self.transform(anchor_waveform, sample_rate).squeeze(0)\n",
    "        positive = self.transform(positive_waveform, sample_rate).squeeze(0)\n",
    "        negative = self.transform(negative_waveform, sample_rate).squeeze(0)\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VTeXhiJpR204"
   },
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.speaker_indices = self._build_speaker_indices()\n",
    "        self.pairs = self._generate_pairs()\n",
    "\n",
    "    def _build_speaker_indices(self):\n",
    "        speaker_indices = {}\n",
    "        for i, (_, _, _, speaker_id, _, _) in enumerate(self.dataset):\n",
    "            if speaker_id not in speaker_indices:\n",
    "                speaker_indices[speaker_id] = []\n",
    "            speaker_indices[speaker_id].append(i)\n",
    "        return speaker_indices\n",
    "\n",
    "    def _generate_pairs(self):\n",
    "        pairs = []\n",
    "        speaker_ids = list(self.speaker_indices.keys())\n",
    "        for speaker_id in speaker_ids:\n",
    "            indices = self.speaker_indices[speaker_id]\n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i + 1, len(indices)):\n",
    "                    pairs.append((indices[i], indices[j], 1))\n",
    "        num_positive_pairs = len(pairs)\n",
    "        num_negative_pairs = 0\n",
    "        while num_negative_pairs < num_positive_pairs:\n",
    "            idx1 = random.choice(range(len(self.dataset)))\n",
    "            idx2 = random.choice(range(len(self.dataset)))\n",
    "            _, _, _, speaker_id1, _, _ = self.dataset[idx1]\n",
    "            _, _, _, speaker_id2, _, _ = self.dataset[idx2]\n",
    "            if speaker_id1 != speaker_id2:\n",
    "                pairs.append((idx1, idx2, 0))\n",
    "                num_negative_pairs += 1\n",
    "        random.shuffle(pairs)\n",
    "        return pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx1, idx2, label = self.pairs[idx]\n",
    "        waveform1, sample_rate1, _, _, _, _ = self.dataset[idx1]\n",
    "        waveform2, sample_rate2, _, _, _, _ = self.dataset[idx2]\n",
    "\n",
    "        sample1 = self.transform(waveform1, sample_rate1).squeeze(0)\n",
    "        sample2 = self.transform(waveform2, sample_rate2).squeeze(0)\n",
    "\n",
    "        return sample1, sample2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DcYTiBJhJEqE"
   },
   "outputs": [],
   "source": [
    "def collate_fn_triplet(batch):\n",
    "    anchors, positives, negatives = zip(*batch)\n",
    "    anchors_padded = pad_sequence(anchors, batch_first=True)\n",
    "    positives_padded = pad_sequence(positives, batch_first=True)\n",
    "    negatives_padded = pad_sequence(negatives, batch_first=True)\n",
    "    return anchors_padded, positives_padded, negatives_padded\n",
    "\n",
    "def collate_fn_eval(batch):\n",
    "    sample1, sample2, labels = zip(*batch)\n",
    "    sample1_padded = pad_sequence(sample1, batch_first=True)\n",
    "    sample2_padded = pad_sequence(sample2, batch_first=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return sample1_padded, sample2_padded, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rmE4h0ZuSO2i"
   },
   "outputs": [],
   "source": [
    "eval_dataset = PairDataset(val_dataset, transform=get_mfcc)\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_eval,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TycHeaKKDosV"
   },
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = F.pairwise_distance(anchor, positive)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative)\n",
    "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "yutkZYzBDrCe"
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, input_dim)\n",
    "        self.v = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = torch.tanh(self.W(x))\n",
    "        scores = self.v(scores)\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        context_vector = torch.sum(attention_weights * x, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "class SpeakerVerificationSiameseNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, freeze_layers=False):\n",
    "        super(SpeakerVerificationSiameseNet, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        if freeze_layers:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.resnet.avgpool = nn.Identity()\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.rnn = nn.GRU(input_size=512, hidden_size=embedding_dim, batch_first=True)\n",
    "        self.attention_layer = AttentionLayer(embedding_dim)\n",
    "        self.projection_layer = nn.Sequential(\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.view(batch_size, x.size(1) * x.size(2), -1)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.attention_layer(x)\n",
    "        x = self.projection_layer(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-yS53afuRctT"
   },
   "outputs": [],
   "source": [
    "subset_size = 5000\n",
    "indices = random.sample(range(len(librispeech_dataset)), subset_size)\n",
    "train_size = int(0.8 * subset_size)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_dataset = Subset(librispeech_dataset, train_indices)\n",
    "val_dataset = Subset(librispeech_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TripletDataset(train_dataset, transform=get_mfcc, augment=True),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_triplet,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TripletDataset(val_dataset, transform=get_mfcc, augment=False),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_triplet,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kHTkOxZyeyfK",
    "outputId": "b1cc9471-d4de-46a8-a51a-e327d26bc6e2"
   },
   "outputs": [],
   "source": [
    "model = SpeakerVerificationSiameseNet(embedding_dim=256).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "loss_fn = TripletLoss(margin=1.0).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "collapsed": true,
    "id": "ViDurBMhep33",
    "outputId": "0a7a1c51-a5b4-4a86-e23e-c88d1e0d39c7"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (anchor, positive, negative) in enumerate(train_loader):\n",
    "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get embeddings for anchor, positive, and negative\n",
    "        anchor_embedding = model(anchor)\n",
    "        positive_embedding = model(positive)\n",
    "        negative_embedding = model(negative)\n",
    "\n",
    "        # Compute triplet loss\n",
    "        loss = loss_fn(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in val_loader:\n",
    "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
    "            anchor_embedding = model(anchor)\n",
    "            positive_embedding = model(positive)\n",
    "            negative_embedding = model(negative)\n",
    "            val_loss += loss_fn(anchor_embedding, positive_embedding, negative_embedding).item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch} completed. Training loss: {avg_train_loss}, Validation loss: {avg_val_loss}')\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_speaker_verification_model.pth')\n",
    "        print(\"Model saved with improved validation loss.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muJJTbnUe2kT"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_speaker_verification_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRY40Ee2sNb7"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_loader, device):\n",
    "    \"\"\"Evaluate the model using ROC and calculate EER.\"\"\"\n",
    "    model.eval()\n",
    "    all_distances, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for anchor, query, label in eval_loader:\n",
    "            anchor, query, label = anchor.to(device), query.to(device), label.to(device)\n",
    "            embedding1 = model(anchor)\n",
    "            embedding2 = model(query)\n",
    "            distance = F.pairwise_distance(embedding1, embedding2)\n",
    "            all_distances.extend(-distance.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    # Calculate ROC Curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_distances)\n",
    "    auc_score = roc_auc_score(all_labels, all_distances)\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, all_distances)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label='Precision-Recall Curve')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate EER\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = brentq(lambda x: 1. - x - interp1d(fpr, fnr)(x), 0., 1.)\n",
    "    eer = interp1d(fpr, thresholds)(eer_threshold)\n",
    "    print(f\"AUC Score: {auc_score:.2f}\")\n",
    "    print(f\"EER: {eer_threshold * 100:.2f}% at threshold {eer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFkU7KdT3G80"
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_samples_by_speaker(dataset, speaker_id, num_samples=2):\n",
    "    indices = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, _, _, spk_id, _, _ = dataset[idx]\n",
    "        if spk_id == speaker_id:\n",
    "            indices.append(idx)\n",
    "            if len(indices) == num_samples:\n",
    "                break\n",
    "    return indices\n",
    "same_speaker_id = librispeech_dataset[0][3]\n",
    "same_speaker_indices = find_samples_by_speaker(librispeech_dataset, same_speaker_id, num_samples=2)\n",
    "diff_speaker_ids = []\n",
    "for idx in range(len(librispeech_dataset)):\n",
    "    _, _, _, spk_id, _, _ = librispeech_dataset[idx]\n",
    "    if spk_id != same_speaker_id and spk_id not in diff_speaker_ids:\n",
    "        diff_speaker_ids.append(spk_id)\n",
    "    if len(diff_speaker_ids) == 2:\n",
    "        break\n",
    "\n",
    "diff_speaker_indices = []\n",
    "for spk_id in diff_speaker_ids:\n",
    "    indices = find_samples_by_speaker(librispeech_dataset, spk_id, num_samples=1)\n",
    "    diff_speaker_indices.extend(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddXQm53_3TOY"
   },
   "outputs": [],
   "source": [
    "\n",
    "waveform1, sample_rate1, _, _, _, _ = librispeech_dataset[same_speaker_indices[0]]\n",
    "waveform2, sample_rate2, _, _, _, _ = librispeech_dataset[same_speaker_indices[1]]\n",
    "\n",
    "mfcc1 = get_mfcc(waveform1, sample_rate1).squeeze(0).to(device)\n",
    "mfcc2 = get_mfcc(waveform2, sample_rate2).squeeze(0).to(device)\n",
    "waveform3, sample_rate3, _, _, _, _ = librispeech_dataset[diff_speaker_indices[0]]\n",
    "waveform4, sample_rate4, _, _, _, _ = librispeech_dataset[diff_speaker_indices[1]]\n",
    "\n",
    "mfcc3 = get_mfcc(waveform3, sample_rate3).squeeze(0).to(device)\n",
    "mfcc4 = get_mfcc(waveform4, sample_rate4).squeeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jl2mmt1x3WzF"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding1 = model(mfcc1.unsqueeze(0))\n",
    "    embedding2 = model(mfcc2.unsqueeze(0))\n",
    "    embedding3 = model(mfcc3.unsqueeze(0))\n",
    "    embedding4 = model(mfcc4.unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRu7Y7j03Y2N"
   },
   "outputs": [],
   "source": [
    "\n",
    "distance_same = F.pairwise_distance(embedding1, embedding2).item()\n",
    "distance_diff1 = F.pairwise_distance(embedding1, embedding3).item()\n",
    "distance_diff2 = F.pairwise_distance(embedding1, embedding4).item()\n",
    "\n",
    "print(f\"Distance between same speaker samples: {distance_same:.4f}\")\n",
    "print(f\"Distance between different speaker samples (Sample 1 and 3): {distance_diff1:.4f}\")\n",
    "print(f\"Distance between different speaker samples (Sample 1 and 4): {distance_diff2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95U0CKEZWUmo"
   },
   "outputs": [],
   "source": [
    "def enroll_user(model, enrollment_audio_path):\n",
    "    \"\"\"\n",
    "    Generate and store the embedding for the enrollment audio.\n",
    "\n",
    "    Args:\n",
    "        model: Trained speaker verification model.\n",
    "        enrollment_audio_path: Path to the enrollment audio file.\n",
    "\n",
    "    Returns:\n",
    "        embedding: The stored embedding for the enrollment audio.\n",
    "    \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(enrollment_audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    mfcc = get_mfcc(waveform, sample_rate=16000).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(mfcc.unsqueeze(0))\n",
    "    return embedding.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8jb_jUuXZuD"
   },
   "outputs": [],
   "source": [
    "def verify_user(model, stored_embedding, verification_audio_path, threshold):\n",
    "    \"\"\"\n",
    "    Verify if the verification audio matches the stored embedding.\n",
    "\n",
    "    Args:\n",
    "        model: Trained speaker verification model.\n",
    "        stored_embedding: The stored embedding from the enrollment phase.\n",
    "        verification_audio_path: Path to the verification audio file.\n",
    "        threshold: Similarity threshold for authentication.\n",
    "\n",
    "    Returns:\n",
    "        is_authenticated (bool): Whether the user is authenticated.\n",
    "        similarity_score (float): The similarity score between the embeddings.\n",
    "    \"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(verification_audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "\n",
    "    mfcc = get_mfcc(waveform, sample_rate=16000).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        current_embedding = model(mfcc.unsqueeze(0))\n",
    "\n",
    "    distance = F.pairwise_distance(stored_embedding, current_embedding).item()\n",
    "    similarity_score = -distance\n",
    "    is_authenticated = similarity_score > threshold\n",
    "    return is_authenticated, similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRoHMvhyXcM6"
   },
   "outputs": [],
   "source": [
    "stored_embedding = enroll_user(model, '/content/enrollment_audio.wav')\n",
    "is_authenticated, score = verify_user(model, stored_embedding, '/content/verification_audio.wav', threshold=0.5)\n",
    "\n",
    "print(f\"Authenticated: {is_authenticated}, Similarity Score: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
